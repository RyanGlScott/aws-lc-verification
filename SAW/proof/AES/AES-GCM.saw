/*
 * Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
 * SPDX-License-Identifier: Apache-2.0
*/

import "../../../cryptol-specs/Primitive/Symmetric/Cipher/Block/AES.cry";
import "../../../cryptol-specs/Primitive/Symmetric/Cipher/Authenticated/AES_256_GCM.cry";
import "../../spec/AES/X86.cry";
import "../../spec/AES/AES-GCM.cry";
import "../../spec/AES/AES-GCM-implementation.cry";
import "../../spec/AES/AES-GCM-unbounded.cry";


enable_experimental;


// Disable debug intrinsics to avoid https://github.com/GaloisInc/crucible/issues/778
disable_debug_intrinsics;

m <- llvm_load_module "../../build/llvm_x86/crypto/crypto_test.bc";


include "../common/helpers.saw";
include "../common/memory.saw";


/*
 * Verification parameters.
 */

// The total input length before the call to the update function.
let evp_cipher_update_gcm_len = 10;

/*
 * The GCM implementation has multiple phases:
 * 1) A "bulk" encryption/decryption that operates on multiples of 6 blocks, and computes
 *    AES/CTR32 and GHASH in parallel using different functional units of the CPU.
 * 2) An optimized AES/CTR32 implementation that processes the remaining blocks.
 * 3) An optimized GHASH implementation that processes the remaining blocks.
 *
 * The code below calculates the correct input lengths for each subroutine.
 */

// If starting auth length is nonzero, bytes are collected and a single GCM_MUL is performed.
// So bulk GCM auth length is the starting length rounded up to the next multiple of 16.
let aesni_gcm_cipher_gcm_len = eval_size {| (evp_cipher_update_gcm_len + 15) / 16 * 16 |};
let gcm_len_diff = eval_size {| aesni_gcm_cipher_gcm_len - evp_cipher_update_gcm_len |};
let aesni_gcm_cipher_len = eval_size {| evp_cipher_update_len - (min gcm_len_diff evp_cipher_update_len) |};

// To get size for bulk encrypt operatin, round down to nearest multiple of 96 (min 288)
let evp_cipher_update_bulk_encrypt = eval_size {| max 288 (aesni_gcm_cipher_len / 96 * 96) |};
let bulk_encrypt_used = eval_size {| aesni_gcm_cipher_len / evp_cipher_update_bulk_encrypt |};
// Separate AES-CTR32 and GHASH encryption on the remaining whole blocks
// When there are no blocks left after the bulk operation, prove the CTR32 and GHASH
// operations correct for 1 block. The code is not entered, so this proof is never used,
// but it avoids complications related to proving these operations correct on 0 blocks.
let length_after_bulk_encrypt = eval_size {| aesni_gcm_cipher_len - bulk_encrypt_used * evp_cipher_update_bulk_encrypt |};
let whole_blocks_after_bulk_encrypt = eval_size {| max 1 (length_after_bulk_encrypt / 16) |};
let GHASH_length_blocks_encrypt = eval_size {| whole_blocks_after_bulk_encrypt * 16|};

// Bulk decrypt core also operates on 6 blocks, but the minimum is 6 blocks.
let evp_cipher_update_bulk_decrypt = eval_size {| max 96 (aesni_gcm_cipher_len / 96 * 96) |};
let bulk_decrypt_used = eval_size {| aesni_gcm_cipher_len / evp_cipher_update_bulk_decrypt |};
// Separate AES-CTR32 and GHASH decryption on the remaining whole blocks
// Limit to minimum of 1 block like in encrypt case.
let length_after_bulk_decrypt = eval_size {| aesni_gcm_cipher_len - bulk_decrypt_used * evp_cipher_update_bulk_decrypt |};
let whole_blocks_after_bulk_decrypt = eval_size {| max 1 (length_after_bulk_decrypt / 16) |};
let GHASH_length_blocks_decrypt = eval_size {| whole_blocks_after_bulk_decrypt * 16|};

let evp_cipher_final_gcm_len = eval_size {| evp_cipher_update_gcm_len + evp_cipher_update_len |};

// Log the calculated values to help find errors when the proof fails.
print (str_concat "evp_cipher_update_len=" (show evp_cipher_update_len));
print (str_concat "aesni_gcm_cipher_gcm_len=" (show aesni_gcm_cipher_gcm_len));
print (str_concat "aesni_gcm_cipher_len=" (show aesni_gcm_cipher_len));

print (str_concat "evp_cipher_update_bulk_encrypt=" (show evp_cipher_update_bulk_encrypt));
print (str_concat "bulk_encrypt_used=" (show bulk_encrypt_used));
print (str_concat "length_after_bulk_encrypt=" (show length_after_bulk_encrypt));
print (str_concat "whole_blocks_after_bulk_encrypt=" (show whole_blocks_after_bulk_encrypt));
print (str_concat "GHASH_length_blocks_encrypt=" (show GHASH_length_blocks_encrypt));

print (str_concat "evp_cipher_update_bulk_decrypt=" (show evp_cipher_update_bulk_decrypt));
print (str_concat "bulk_decrypt_used=" (show bulk_decrypt_used));
print (str_concat "length_after_bulk_decrypt=" (show length_after_bulk_decrypt));
print (str_concat "whole_blocks_after_bulk_decrypt=" (show whole_blocks_after_bulk_decrypt));
print (str_concat "GHASH_length_blocks_decrypt=" (show GHASH_length_blocks_decrypt));


include "goal-rewrites.saw";


let NID_aes_256_gcm = 901;
let aes_block_size = 1;
// the IV for AES-GCM consists of 12 32-bit integers
let aes_iv_len = 12;

// This computes the total number of message blocks that can be
// handeled by a single AES/GCM mode session. The GCM counter is
// a 32-bit counter which starts at 1, and we need to leave a block at
// the end for the authentication tag. This gives us a total of
// slightly fewer than 2^^32 blocks we can handle.
let TOTAL_MESSAGE_BLOCKS = eval_size {| 2^^32 - 2 |};

// This is the minimum number of blocks that can be processed by the bulk
// encrypt/decrypt phase.  This is due to the fact that the bulk encryption
// phase processes 6 block chunks, and has a pipeline setup which is three
// stages deep. Thus, 18 blocks is the minimum number of blocks it will
// process; fewer than that and it will simply rely on the separate AES/CTR32
// and GHASH routines.
let MIN_BULK_BLOCKS = 18;


/*
 * Architecture features for the AVX+shrd code path
 * ia32cap set to disable AVX512[F|DQ|BW|VL] instructions
 * https://www.openssl.org/docs/manmaster/man3/OPENSSL_ia32cap.html
 */
let {{ ia32cap = [0xffffffff, 0xffffffff, 0x3ffcffff, 0xffffffff] : [4][32] }};


include "AES.saw";
include "GHASH.saw";
include "AES-CTR32.saw";
include "AESNI-GCM.saw";


////////////////////////////////////////////////////////////////////////////////
// Specifications

let EVP_AES_GCM_CTX_PADDING = 8;
let EVP_AES_GCM_CTX_size = llvm_sizeof m (llvm_struct "struct.EVP_AES_GCM_CTX");
let ctx_size = eval_size {| EVP_AES_GCM_CTX_size + EVP_AES_GCM_CTX_PADDING |};


/*
 * Helpers for specifying the AES-GCM structs
 */
let EVP_CIPH_GCM_MODE = 0x6;
let EVP_CIPH_ALWAYS_CALL_INIT = 0x80;
let EVP_CIPH_CUSTOM_IV = 0x100;
let EVP_CIPH_CTRL_INIT = 0x200;
let EVP_CIPH_FLAG_CUSTOM_CIPHER = 0x400;
let EVP_CIPH_FLAG_AEAD_CIPHER = 0x800;
let EVP_CIPH_CUSTOM_COPY = 0x1000;

// This is the total number of bytes that can be in the plain/cyphertext
// for AES-GCM.
let TOTAL_MESSAGE_MAX_LENGTH = eval_size {| TOTAL_MESSAGE_BLOCKS * AES_BLOCK_SIZE |};

let points_to_evp_cipher_st ptr = do {
  crucible_points_to (crucible_elem ptr 0) (crucible_term {{ `NID_aes_256_gcm : [32] }});
  crucible_points_to (crucible_elem ptr 1) (crucible_term {{ `aes_block_size : [32] }});
  crucible_points_to (crucible_elem ptr 2) (crucible_term {{ `aes_key_len : [32] }});
  crucible_points_to (crucible_elem ptr 3) (crucible_term {{ `aes_iv_len : [32] }});
  crucible_points_to (crucible_elem ptr 4) (crucible_term {{ `ctx_size : [32] }});
  let flags = eval_size {| EVP_CIPH_GCM_MODE + EVP_CIPH_CUSTOM_IV + EVP_CIPH_CUSTOM_COPY +
                           EVP_CIPH_FLAG_CUSTOM_CIPHER + EVP_CIPH_ALWAYS_CALL_INIT +
                           EVP_CIPH_CTRL_INIT + EVP_CIPH_FLAG_AEAD_CIPHER |};
  crucible_points_to (crucible_elem ptr 5) (crucible_term {{ `flags : [32] }});
  crucible_points_to (crucible_elem ptr 6) crucible_null;
  crucible_points_to (crucible_elem ptr 7) (crucible_global "aes_gcm_init_key");
  crucible_points_to (crucible_elem ptr 8) (crucible_global "aes_gcm_cipher");
  crucible_points_to (crucible_elem ptr 9) (crucible_global "aes_gcm_cleanup");
  crucible_points_to (crucible_elem ptr 10) (crucible_global "aes_gcm_ctrl");
};

let points_to_evp_cipher_ctx_st ptr cipher_ptr cipher_data_ptr enc = do {
  crucible_points_to (crucible_field ptr "cipher") cipher_ptr;
  crucible_points_to (crucible_field ptr "cipher_data") cipher_data_ptr;
  crucible_points_to (crucible_field ptr "key_len") (crucible_term {{ `aes_key_len : [32] }});
  crucible_points_to (crucible_field ptr "encrypt") (crucible_term enc);
  crucible_points_to (crucible_field ptr "flags") (crucible_term {{ 0 : [32] }});
  crucible_points_to (crucible_field ptr "buf_len") (crucible_term {{ 0 : [32] }});
  crucible_points_to (crucible_field ptr "final_used") (crucible_term {{ 0 : [32] }});
  crucible_points_to (crucible_field ptr "poisoned") (crucible_term {{ 0 : [32] }});
};

let fresh_aes_gcm_ctx len = do {
  key <- fresh_aes_key_st;
  iv <- crucible_fresh_var "iv" (llvm_array aes_iv_len (llvm_int 8));
  Xi <- crucible_fresh_var "Xi" (llvm_array AES_BLOCK_SIZE (llvm_int 8));
  return {{ { key = key, iv = iv, Xi = Xi, len = `len } : AES_GCM_Ctx }};
};

let fresh_aes_gcm_ctx' mres = do {
  key <- fresh_aes_key_st;
  iv <- crucible_fresh_var "iv" (llvm_array aes_iv_len (llvm_int 8));
  Xi <- crucible_fresh_var "Xi" (llvm_array AES_BLOCK_SIZE (llvm_int 8));
  len_60 <- llvm_fresh_var "ctx.len" (llvm_int 60);
  let len = {{ (len_60 # `mres): [64] }};
  return {{ { key = key, iv = iv, Xi = Xi, len = len } : AES_GCM_Ctx }};
};

let points_to_gcm128_key_st ptr ctx = do {
  crucible_points_to_untyped (crucible_elem ptr 0) (crucible_term {{ get_Htable ctx.key }});
  crucible_points_to (crucible_elem ptr 1) (crucible_global "gcm_gmult_avx");
  crucible_points_to (crucible_elem ptr 2) (crucible_global "gcm_ghash_avx");
  crucible_points_to (crucible_elem ptr 3) (crucible_global "aes_hw_encrypt");
  crucible_points_to (crucible_elem ptr 4) (crucible_term {{ 1 : [8] }});
};

let points_to_GCM128_CONTEXT ptr ctx mres = do {
  crucible_points_to_untyped (crucible_elem ptr 0) (crucible_term {{ get_Yi ctx }});
  if eval_bool {{ `mres == 0 }} then do {
    return ();
  } else do {
    crucible_points_to_untyped (crucible_elem ptr 1) (crucible_term {{ get_EKi ctx }});
  };
  crucible_points_to_untyped (crucible_elem ptr 2) (crucible_term {{ get_EK0 ctx }});
  crucible_points_to_untyped (crucible_elem ptr 3) (crucible_term {{ [(0 : [64]), ctx.len] }});
  crucible_points_to_untyped (crucible_elem ptr 4) (crucible_term {{ ctx.Xi }});
  points_to_gcm128_key_st (crucible_elem ptr 5) ctx;
  crucible_points_to (crucible_elem ptr 6) (crucible_term {{ `mres : [32] }});
  crucible_points_to (crucible_elem ptr 7) (crucible_term {{ 0 : [32] }});
};

let points_to_EVP_AES_GCM_CTX ptr ctx mres iv_set taglen = do {
  points_to_GCM128_CONTEXT (crucible_field ptr "gcm") ctx mres;
  points_to_aes_key_st (crucible_field ptr "ks") {{ ctx.key }};
  crucible_points_to (crucible_field ptr "key_set") (crucible_term {{ 1 : [32] }});
  crucible_points_to (crucible_field ptr "iv_set") (crucible_term iv_set);
  crucible_points_to (crucible_field ptr "ivlen") (crucible_term {{ `aes_iv_len : [32] }});
  crucible_points_to (crucible_field ptr "taglen") (crucible_term {{ `taglen : [32] }});
  crucible_points_to (crucible_field ptr "iv_gen") (crucible_term {{ 0 : [32] }});
  crucible_points_to (crucible_field ptr "ctr") (crucible_global "aes_hw_ctr32_encrypt_blocks");
};


let aes_gcm_from_cipher_ctx_spec = do {
  cipher_data_ptr <- crucible_alloc_readonly_aligned 16 (llvm_struct "struct.EVP_AES_GCM_CTX");

  crucible_execute_func [cipher_data_ptr];

  crucible_return cipher_data_ptr;
};


include "evp-function-specs.saw";


////////////////////////////////////////////////////////////////////////////////
// Proof commands

aes_gcm_from_cipher_ctx_ov <- crucible_llvm_unsafe_assume_spec
  m
  "aes_gcm_from_cipher_ctx"
  aes_gcm_from_cipher_ctx_spec;


llvm_verify m "EVP_aes_256_gcm_init" [] true EVP_aes_256_gcm_init_spec (w4_unint_yices []);


let evp_cipher_ovs =
  [ OPENSSL_malloc_ov
  , aes_gcm_from_cipher_ctx_ov
  , aes_hw_set_encrypt_key_ov
  , aes_hw_encrypt_ov
  , aes_hw_encrypt_in_place_ov
  , aes_hw_ctr32_encrypt_blocks_encrypt_ov
  , aes_hw_ctr32_encrypt_blocks_decrypt_ov
  , gcm_init_avx_ov
  , gcm_gmult_avx_ov
  , gcm_ghash_avx_encrypt_ov
  , gcm_ghash_avx_decrypt_ov
  , aesni_gcm_encrypt_ov
  , aesni_gcm_decrypt_ov
  ];


llvm_verify m "EVP_CipherInit_ex"
  evp_cipher_ovs
  true
  (EVP_CipherInit_ex_spec {{ 1 : [32] }})
  evp_cipher_tactic;

llvm_verify m "EVP_CipherInit_ex"
  evp_cipher_ovs
  true
  (EVP_CipherInit_ex_spec {{ 0 : [32] }})
  evp_cipher_tactic;


enable_what4_hash_consing;

llvm_verify m "EVP_EncryptUpdate"
  evp_cipher_ovs
  true
  (EVP_CipherUpdate_spec {{ 1 : [32] }} evp_cipher_update_gcm_len evp_cipher_update_len)
  evp_cipher_tactic;

llvm_verify m "EVP_DecryptUpdate"
  evp_cipher_ovs
  true
  (EVP_CipherUpdate_spec {{ 0 : [32] }} evp_cipher_update_gcm_len evp_cipher_update_len)
  evp_cipher_tactic;

enable_what4_eval;

llvm_verify m "EVP_EncryptUpdate"
  [ aes_gcm_from_cipher_ctx_ov
  , aes_hw_encrypt_ov
  , aes_hw_ctr32_encrypt_blocks_bounded_array_ov
  , gcm_gmult_avx_ov
  , gcm_ghash_avx_bounded_array_ov
  , aesni_gcm_encrypt_array_ov
  , aesni_gcm_decrypt_array_ov
  ]
  true
  (EVP_CipherUpdate_array_spec {{ 1 : [32] }} 1 15)
  (do {
    simplify (cryptol_ss ());
    goal_eval_unint ["aesni_gcm_encrypt", "aes_ctr32_encrypt_blocks_array", "aes_hw_encrypt", "gcm_ghash_blocks_array", "gcm_polyval", "pmult", "pmod"];
    simplify (addsimps [aesni_gcm_encrypt_Yi_thm] empty_ss);
    goal_eval_unint ["aesni_gcm_encrypt", "aes_ctr32_encrypt_blocks_array", "aes_hw_encrypt", "gcm_ghash_blocks_array", "gcm_polyval", "pmult", "pmod"];
    simplify (addsimps [bvand_bvudiv_thm, bvudiv_bvmul_bvudiv_thm, bvurem_16_append_thm] basic_ss);
    simplify (addsimps [arrayLookupUnint_thm, arrayUpdateUnint_thm, arrayCopyUnint_thm, arrayConstantUnint_thm] empty_ss);
    simplify (addsimps add_xor_slice_thms basic_ss);
    goal_eval_unint ["aesni_gcm_encrypt", "aes_ctr32_encrypt_blocks_array", "aes_hw_encrypt", "gcm_ghash_blocks_array", "gcm_polyval", "pmult", "pmod", "arrayLookupUnint", "arrayUpdateUnint", "arrayCopyUnint", "arrayConstantUnint"];
    simplify (addsimps [ite_bveq_0_thm, ite_bveq_1_thm, bveq_ite_bv8_0_thm, bveq_ite_bv8_1_thm, arrayeq_ite_0_thm, arrayeq_ite_1_thm, foo_thm, bar_thm] basic_ss);
    goal_eval_unint ["aesni_gcm_encrypt", "aes_ctr32_encrypt_blocks_array", "aes_hw_encrypt", "gcm_ghash_blocks_array", "gcm_polyval", "pmult", "pmod", "arrayLookupUnint", "arrayUpdateUnint", "arrayCopyUnint", "arrayConstantUnint"];
    print_goal;
    is_out_post <- goal_has_some_tag ["output buffer postcondition"];
    is_Xi_post <- goal_has_some_tag ["Xi postcondition"];
    if is_out_post then do {
      w4_unint_yices ["aesni_gcm_encrypt", "aes_ctr32_encrypt_blocks_array", "aes_hw_encrypt", "gcm_ghash_blocks_array", "gcm_polyval", "pmult", "pmod", "arrayLookupUnint", "arrayUpdateUnint", "arrayCopyUnint", "arrayConstantUnint"];
    } else if is_Xi_post then do {
      w4_unint_z3_using "qfufbv" ["aesni_gcm_encrypt", "aes_ctr32_encrypt_blocks_array", "aes_hw_encrypt", "gcm_ghash_blocks_array", "gcm_polyval", "pmult", "pmod", "arrayLookupUnint", "arrayUpdateUnint", "arrayCopyUnint", "arrayConstantUnint"];
    } else do {
      w4_unint_z3 ["aesni_gcm_encrypt", "aes_ctr32_encrypt_blocks_array", "aes_hw_encrypt", "gcm_ghash_blocks_array", "gcm_polyval", "pmult", "pmod", "arrayLookupUnint", "arrayUpdateUnint", "arrayCopyUnint", "arrayConstantUnint"];
    };
  });

llvm_verify m "EVP_DecryptUpdate"
  [ aes_gcm_from_cipher_ctx_ov
  , aes_hw_encrypt_ov
  , aes_hw_ctr32_encrypt_blocks_bounded_array_ov
  , gcm_gmult_avx_ov
  , gcm_ghash_avx_bounded_array_ov
  , aesni_gcm_encrypt_array_ov
  , aesni_gcm_decrypt_array_ov
  ]
  true
  (EVP_CipherUpdate_array_spec {{ 0 : [32] }} 1 15)
  (do {
    simplify (cryptol_ss ());
    goal_eval_unint ["aesni_gcm_decrypt", "aes_ctr32_encrypt_blocks_array", "aes_hw_encrypt", "gcm_ghash_blocks_array", "gcm_polyval", "pmult", "pmod"];
    simplify (addsimps [aesni_gcm_decrypt_Yi_thm] empty_ss);
    goal_eval_unint ["aesni_gcm_decrypt", "aes_ctr32_encrypt_blocks_array", "aes_hw_encrypt", "gcm_ghash_blocks_array", "gcm_polyval", "pmult", "pmod"];
    simplify (addsimps [bvand_bvudiv_thm, bvudiv_bvmul_bvudiv_thm, bvurem_16_append_thm] basic_ss);
    simplify (addsimps [arrayLookupUnint_thm, arrayUpdateUnint_thm, arrayCopyUnint_thm, arrayConstantUnint_thm] empty_ss);
    simplify (addsimps add_xor_slice_thms basic_ss);
    goal_eval_unint ["aesni_gcm_decrypt", "aes_ctr32_encrypt_blocks_array", "aes_hw_encrypt", "gcm_ghash_blocks_array", "gcm_polyval", "pmult", "pmod", "arrayLookupUnint", "arrayUpdateUnint", "arrayCopyUnint", "arrayConstantUnint"];
    simplify (addsimps [ite_bveq_0_thm, ite_bveq_1_thm, bveq_ite_bv8_0_thm, bveq_ite_bv8_1_thm, arrayeq_ite_0_thm, arrayeq_ite_1_thm, foo_thm, bar_thm] basic_ss);
    goal_eval_unint ["aesni_gcm_decrypt", "aes_ctr32_encrypt_blocks_array", "aes_hw_encrypt", "gcm_ghash_blocks_array", "gcm_polyval", "pmult", "pmod", "arrayLookupUnint", "arrayUpdateUnint", "arrayCopyUnint", "arrayConstantUnint"];
    print_goal;
    is_out_post <- goal_has_some_tag ["output buffer postcondition"];
    is_Xi_post <- goal_has_some_tag ["Xi postcondition"];
    if is_out_post then do {
      w4_unint_yices ["aesni_gcm_decrypt", "aes_ctr32_encrypt_blocks_array", "aes_hw_encrypt", "gcm_ghash_blocks_array", "gcm_polyval", "pmult", "pmod", "arrayLookupUnint", "arrayUpdateUnint", "arrayCopyUnint", "arrayConstantUnint"];
    } else if is_Xi_post then do {
      w4_unint_z3_using "qfufbv" ["aesni_gcm_decrypt", "aes_ctr32_encrypt_blocks_array", "aes_hw_encrypt", "gcm_ghash_blocks_array", "gcm_polyval", "pmult", "pmod", "arrayLookupUnint", "arrayUpdateUnint", "arrayCopyUnint", "arrayConstantUnint"];
    } else do {
      w4_unint_z3 ["aesni_gcm_decrypt", "aes_ctr32_encrypt_blocks_array", "aes_hw_encrypt", "gcm_ghash_blocks_array", "gcm_polyval", "pmult", "pmod", "arrayLookupUnint", "arrayUpdateUnint", "arrayCopyUnint", "arrayConstantUnint"];
    };
  });

disable_what4_eval;
disable_what4_hash_consing;


llvm_verify m "EVP_EncryptFinal_ex"
  evp_cipher_ovs
  true
  (EVP_EncryptFinal_ex_spec evp_cipher_final_gcm_len)
  evp_cipher_tactic;

llvm_verify m "EVP_DecryptFinal_ex"
  evp_cipher_ovs
  true
  (EVP_DecryptFinal_ex_spec evp_cipher_final_gcm_len)
  evp_cipher_tactic;
