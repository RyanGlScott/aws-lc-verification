/*
 * Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
 * SPDX-License-Identifier: Apache-2.0
*/

import "../../../cryptol-specs/Primitive/Symmetric/Cipher/Block/AES.cry";
import "../../../cryptol-specs/Primitive/Symmetric/Cipher/Authenticated/AES_256_GCM.cry";
import "../../spec/AES/X86.cry";
import "../../spec/AES/AES-GCM.cry";
import "../../spec/AES/AES-GCM-implementation.cry";
import "../../spec/AES/AES-GCM-unbounded.cry";


enable_experimental;


// Disable debug intrinsics to avoid https://github.com/GaloisInc/crucible/issues/778
disable_debug_intrinsics;

m <- llvm_load_module "../../build/llvm_x86/crypto/crypto_test.bc";


include "../common/helpers.saw";
include "../common/memory.saw";

let do_prove = false;

/*
 * Verification parameters.
 */

// The total input length before the call to the update function.
let evp_cipher_update_gcm_len = 10;

/*
 * The GCM implementation has multiple phases:
 * 1) A "bulk" encryption/decryption that operates on multiples of 6 blocks, and computes
 *    AES/CTR32 and GHASH in parallel using different functional units of the CPU.
 * 2) An optimized AES/CTR32 implementation that processes the remaining blocks.
 * 3) An optimized GHASH implementation that processes the remaining blocks.
 *
 * The code below calculates the correct input lengths for each subroutine.
 */

// If starting auth length is nonzero, bytes are collected and a single GCM_MUL is performed.
// So bulk GCM auth length is the starting length rounded up to the next multiple of 16.
let aesni_gcm_cipher_gcm_len = eval_size {| (evp_cipher_update_gcm_len + 15) / 16 * 16 |};
let gcm_len_diff = eval_size {| aesni_gcm_cipher_gcm_len - evp_cipher_update_gcm_len |};
let aesni_gcm_cipher_len = eval_size {| evp_cipher_update_len - (min gcm_len_diff evp_cipher_update_len) |};

// To get size for bulk encrypt operatin, round down to nearest multiple of 96 (min 288)
let evp_cipher_update_bulk_encrypt = eval_size {| max 288 (aesni_gcm_cipher_len / 96 * 96) |};
let bulk_encrypt_used = eval_size {| aesni_gcm_cipher_len / evp_cipher_update_bulk_encrypt |};
// Separate AES-CTR32 and GHASH encryption on the remaining whole blocks
// When there are no blocks left after the bulk operation, prove the CTR32 and GHASH
// operations correct for 1 block. The code is not entered, so this proof is never used,
// but it avoids complications related to proving these operations correct on 0 blocks.
let length_after_bulk_encrypt = eval_size {| aesni_gcm_cipher_len - bulk_encrypt_used * evp_cipher_update_bulk_encrypt |};
let whole_blocks_after_bulk_encrypt = eval_size {| max 1 (length_after_bulk_encrypt / 16) |};
let GHASH_length_blocks_encrypt = eval_size {| whole_blocks_after_bulk_encrypt * 16|};

// Bulk decrypt core also operates on 6 blocks, but the minimum is 6 blocks.
let evp_cipher_update_bulk_decrypt = eval_size {| max 96 (aesni_gcm_cipher_len / 96 * 96) |};
let bulk_decrypt_used = eval_size {| aesni_gcm_cipher_len / evp_cipher_update_bulk_decrypt |};
// Separate AES-CTR32 and GHASH decryption on the remaining whole blocks
// Limit to minimum of 1 block like in encrypt case.
let length_after_bulk_decrypt = eval_size {| aesni_gcm_cipher_len - bulk_decrypt_used * evp_cipher_update_bulk_decrypt |};
let whole_blocks_after_bulk_decrypt = eval_size {| max 1 (length_after_bulk_decrypt / 16) |};
let GHASH_length_blocks_decrypt = eval_size {| whole_blocks_after_bulk_decrypt * 16|};

let evp_cipher_final_gcm_len = eval_size {| evp_cipher_update_gcm_len + evp_cipher_update_len |};

// Log the calculated values to help find errors when the proof fails.
print (str_concat "evp_cipher_update_len=" (show evp_cipher_update_len));
print (str_concat "aesni_gcm_cipher_gcm_len=" (show aesni_gcm_cipher_gcm_len));
print (str_concat "aesni_gcm_cipher_len=" (show aesni_gcm_cipher_len));

print (str_concat "evp_cipher_update_bulk_encrypt=" (show evp_cipher_update_bulk_encrypt));
print (str_concat "bulk_encrypt_used=" (show bulk_encrypt_used));
print (str_concat "length_after_bulk_encrypt=" (show length_after_bulk_encrypt));
print (str_concat "whole_blocks_after_bulk_encrypt=" (show whole_blocks_after_bulk_encrypt));
print (str_concat "GHASH_length_blocks_encrypt=" (show GHASH_length_blocks_encrypt));

print (str_concat "evp_cipher_update_bulk_decrypt=" (show evp_cipher_update_bulk_decrypt));
print (str_concat "bulk_decrypt_used=" (show bulk_decrypt_used));
print (str_concat "length_after_bulk_decrypt=" (show length_after_bulk_decrypt));
print (str_concat "whole_blocks_after_bulk_decrypt=" (show whole_blocks_after_bulk_decrypt));
print (str_concat "GHASH_length_blocks_decrypt=" (show GHASH_length_blocks_decrypt));


include "goal-rewrites.saw";


let NID_aes_256_gcm = 901;
let aes_block_size = 1;
// the IV for AES-GCM consists of 12 32-bit integers
let aes_iv_len = 12;

// This computes the total number of message blocks that can be
// handeled by a single AES/GCM mode session. The GCM counter is
// a 32-bit counter which starts at 1, and we need to leave a block at
// the end for the authentication tag. This gives us a total of
// slightly fewer than 2^^32 blocks we can handle.
let TOTAL_MESSAGE_BLOCKS = eval_size {| 2^^32 - 2 |};

// This is the minimum number of blocks that can be processed by the bulk
// encrypt/decrypt phase.  This is due to the fact that the bulk encryption
// phase processes 6 block chunks, and has a pipeline setup which is three
// stages deep. Thus, 18 blocks is the minimum number of blocks it will
// process; fewer than that and it will simply rely on the separate AES/CTR32
// and GHASH routines.
let MIN_BULK_BLOCKS = 18;


/*
 * Architecture features for the AVX+shrd code path
 * ia32cap set to disable AVX512[F|DQ|BW|VL] instructions
 * https://www.openssl.org/docs/manmaster/man3/OPENSSL_ia32cap.html
 */
let {{ ia32cap = [0xffffffff, 0xffffffff, 0x3ffcffff, 0xffffffff] : [4][32] }};


include "AES.saw";
include "GHASH.saw";
include "AES-CTR32.saw";
include "AESNI-GCM.saw";


////////////////////////////////////////////////////////////////////////////////
// Specifications

let EVP_AES_GCM_CTX_PADDING = 8;
let EVP_AES_GCM_CTX_size = llvm_sizeof m (llvm_struct "struct.EVP_AES_GCM_CTX");
let ctx_size = eval_size {| EVP_AES_GCM_CTX_size + EVP_AES_GCM_CTX_PADDING |};


/*
 * Helpers for specifying the AES-GCM structs
 */
let EVP_CIPH_GCM_MODE = 0x6;
let EVP_CIPH_ALWAYS_CALL_INIT = 0x80;
let EVP_CIPH_CUSTOM_IV = 0x100;
let EVP_CIPH_CTRL_INIT = 0x200;
let EVP_CIPH_FLAG_CUSTOM_CIPHER = 0x400;
let EVP_CIPH_FLAG_AEAD_CIPHER = 0x800;
let EVP_CIPH_CUSTOM_COPY = 0x1000;

// This is the total number of bytes that can be in the plain/cyphertext
// for AES-GCM.
let TOTAL_MESSAGE_MAX_LENGTH = eval_size {| TOTAL_MESSAGE_BLOCKS * AES_BLOCK_SIZE |};

let points_to_evp_cipher_st ptr = do {
  crucible_points_to (crucible_elem ptr 0) (crucible_term {{ `NID_aes_256_gcm : [32] }});
  crucible_points_to (crucible_elem ptr 1) (crucible_term {{ `aes_block_size : [32] }});
  crucible_points_to (crucible_elem ptr 2) (crucible_term {{ `aes_key_len : [32] }});
  crucible_points_to (crucible_elem ptr 3) (crucible_term {{ `aes_iv_len : [32] }});
  crucible_points_to (crucible_elem ptr 4) (crucible_term {{ `ctx_size : [32] }});
  let flags = eval_size {| EVP_CIPH_GCM_MODE + EVP_CIPH_CUSTOM_IV + EVP_CIPH_CUSTOM_COPY +
                           EVP_CIPH_FLAG_CUSTOM_CIPHER + EVP_CIPH_ALWAYS_CALL_INIT +
                           EVP_CIPH_CTRL_INIT + EVP_CIPH_FLAG_AEAD_CIPHER |};
  crucible_points_to (crucible_elem ptr 5) (crucible_term {{ `flags : [32] }});
  crucible_points_to (crucible_elem ptr 6) crucible_null;
  crucible_points_to (crucible_elem ptr 7) (crucible_global "aes_gcm_init_key");
  crucible_points_to (crucible_elem ptr 8) (crucible_global "aes_gcm_cipher");
  crucible_points_to (crucible_elem ptr 9) (crucible_global "aes_gcm_cleanup");
  crucible_points_to (crucible_elem ptr 10) (crucible_global "aes_gcm_ctrl");
};

let points_to_evp_cipher_ctx_st ptr cipher_ptr cipher_data_ptr enc = do {
  crucible_points_to (crucible_field ptr "cipher") cipher_ptr;
  crucible_points_to (crucible_field ptr "cipher_data") cipher_data_ptr;
  crucible_points_to (crucible_field ptr "key_len") (crucible_term {{ `aes_key_len : [32] }});
  crucible_points_to (crucible_field ptr "encrypt") (crucible_term enc);
  crucible_points_to (crucible_field ptr "flags") (crucible_term {{ 0 : [32] }});
  crucible_points_to (crucible_field ptr "buf_len") (crucible_term {{ 0 : [32] }});
  crucible_points_to (crucible_field ptr "final_used") (crucible_term {{ 0 : [32] }});
  crucible_points_to (crucible_field ptr "poisoned") (crucible_term {{ 0 : [32] }});
};

let fresh_aes_gcm_ctx len = do {
  key <- fresh_aes_key_st;
  iv <- crucible_fresh_var "iv" (llvm_array aes_iv_len (llvm_int 8));
  Xi <- crucible_fresh_var "Xi" (llvm_array AES_BLOCK_SIZE (llvm_int 8));
  return {{ { key = key, iv = iv, Xi = Xi, len = `len } : AES_GCM_Ctx }};
};

let fresh_aes_gcm_ctx' mres = do {
  key <- fresh_aes_key_st;
  iv <- crucible_fresh_var "iv" (llvm_array aes_iv_len (llvm_int 8));
  Xi <- crucible_fresh_var "Xi" (llvm_array AES_BLOCK_SIZE (llvm_int 8));
  len_60 <- llvm_fresh_var "ctx.len" (llvm_int 60);
  let len = {{ (len_60 # `mres): [64] }};
  return {{ { key = key, iv = iv, Xi = Xi, len = len } : AES_GCM_Ctx }};
};

let points_to_gcm128_key_st ptr ctx = do {
  crucible_points_to_untyped (crucible_elem ptr 0) (crucible_term {{ get_Htable ctx.key }});
  crucible_points_to (crucible_elem ptr 1) (crucible_global "gcm_gmult_avx");
  crucible_points_to (crucible_elem ptr 2) (crucible_global "gcm_ghash_avx");
  crucible_points_to (crucible_elem ptr 3) (crucible_global "aes_hw_encrypt");
  crucible_points_to (crucible_elem ptr 4) (crucible_term {{ 1 : [8] }});
};

let points_to_GCM128_CONTEXT ptr ctx mres = do {
  crucible_points_to_untyped (crucible_elem ptr 0) (crucible_term {{ get_Yi ctx }});
  if eval_bool {{ `mres == 0 }} then do {
    return ();
  } else do {
    crucible_points_to_untyped (crucible_elem ptr 1) (crucible_term {{ get_EKi ctx }});
  };
  crucible_points_to_untyped (crucible_elem ptr 2) (crucible_term {{ get_EK0 ctx }});
  crucible_points_to_untyped (crucible_elem ptr 3) (crucible_term {{ [(0 : [64]), ctx.len] }});
  crucible_points_to_untyped (crucible_elem ptr 4) (crucible_term {{ ctx.Xi }});
  points_to_gcm128_key_st (crucible_elem ptr 5) ctx;
  crucible_points_to (crucible_elem ptr 6) (crucible_term {{ `mres : [32] }});
  crucible_points_to (crucible_elem ptr 7) (crucible_term {{ 0 : [32] }});
};

let points_to_EVP_AES_GCM_CTX ptr ctx mres iv_set taglen = do {
  points_to_GCM128_CONTEXT (crucible_field ptr "gcm") ctx mres;
  points_to_aes_key_st (crucible_field ptr "ks") {{ ctx.key }};
  crucible_points_to (crucible_field ptr "key_set") (crucible_term {{ 1 : [32] }});
  crucible_points_to (crucible_field ptr "iv_set") (crucible_term iv_set);
  crucible_points_to (crucible_field ptr "ivlen") (crucible_term {{ `aes_iv_len : [32] }});
  crucible_points_to (crucible_field ptr "taglen") (crucible_term {{ `taglen : [32] }});
  crucible_points_to (crucible_field ptr "iv_gen") (crucible_term {{ 0 : [32] }});
  crucible_points_to (crucible_field ptr "ctr") (crucible_global "aes_hw_ctr32_encrypt_blocks");
};


let aes_gcm_from_cipher_ctx_spec = do {
  cipher_data_ptr <- crucible_alloc_readonly_aligned 16 (llvm_struct "struct.EVP_AES_GCM_CTX");

  crucible_execute_func [cipher_data_ptr];

  crucible_return cipher_data_ptr;
};


include "evp-function-specs.saw";


////////////////////////////////////////////////////////////////////////////////
// Proof commands

aes_gcm_from_cipher_ctx_ov <- crucible_llvm_unsafe_assume_spec
  m
  "aes_gcm_from_cipher_ctx"
  aes_gcm_from_cipher_ctx_spec;


llvm_verify m "EVP_aes_256_gcm_init" [] true EVP_aes_256_gcm_init_spec (w4_unint_yices []);


let evp_cipher_ovs =
  [ OPENSSL_malloc_ov
  , aes_gcm_from_cipher_ctx_ov
  , aes_hw_set_encrypt_key_ov
  , aes_hw_encrypt_ov
  , aes_hw_encrypt_in_place_ov
  , aes_hw_ctr32_encrypt_blocks_encrypt_ov
  , aes_hw_ctr32_encrypt_blocks_decrypt_ov
  , gcm_init_avx_ov
  , gcm_gmult_avx_ov
  , gcm_ghash_avx_encrypt_ov
  , gcm_ghash_avx_decrypt_ov
  , aesni_gcm_encrypt_ov
  , aesni_gcm_decrypt_ov
  ];


llvm_verify m "EVP_CipherInit_ex"
  evp_cipher_ovs
  true
  (EVP_CipherInit_ex_spec {{ 1 : [32] }})
  evp_cipher_tactic;

llvm_verify m "EVP_CipherInit_ex"
  evp_cipher_ovs
  true
  (EVP_CipherInit_ex_spec {{ 0 : [32] }})
  evp_cipher_tactic;


enable_what4_hash_consing;

llvm_verify m "EVP_EncryptUpdate"
  evp_cipher_ovs
  true
  (EVP_CipherUpdate_spec {{ 1 : [32] }} evp_cipher_update_gcm_len evp_cipher_update_len)
  evp_cipher_tactic;

llvm_verify m "EVP_DecryptUpdate"
  evp_cipher_ovs
  true
  (EVP_CipherUpdate_spec {{ 0 : [32] }} evp_cipher_update_gcm_len evp_cipher_update_len)
  evp_cipher_tactic;

enable_what4_eval;

// lolwut <- prove_print assume_unsat
//  (parse_core
//   "\\(n1 n2 : Nat) -> \\(arr : Array (Vec 64 Bool) (Vec 8 Bool)) -> \\(v1 v2 v3 : Vec (addNat (addNat n1 8) n2) Bool) -> \\(atIdx : Nat) -> \\(b : Bool) -> \\(aes1 aes2 : Vec 16 (Vec 8 Bool)) -> \\(z1 z2 z3 z4 : Vec 64 Bool) -> \\(arrayLookupUnint : Array (Vec 64 Bool) (Vec 8 Bool) -> Vec 64 Bool -> Vec 8 Bool) -> \
//   \ boolEq \
//   \  (bvEq 8 \
//   \     (bvXor 8 \
//   \        (bvXor 8 (at 16 (Vec 8 Bool) aes1 atIdx) \
//   \           (arrayLookupUnint arr \
//   \              (bvAdd 64 z1 z2))) \
//   \        (ite (Vec 8 Bool) b (slice Bool n1 8 n2 v1) \
//   \           (slice Bool n1 8 n2 v2))) \
//   \     (bvXor 8 \
//   \        (bvXor 8 (at 16 (Vec 8 Bool) aes2 atIdx) \
//   \           (arrayLookupUnint arr (bvAdd 64 z3 z4))) \
//   \        (slice Bool n1 8 n2 v3))) \
//   \  (bvEq 8 \
//   \     (bvXor 8 \
//   \        (bvXor 8 (at 16 (Vec 8 Bool) aes1 atIdx) \
//   \           (arrayLookupUnint arr (bvAdd 64 z3 z4))) \
//   \        (slice Bool n1 8 n2 v3)) \
//   \     (bvXor 8 \
//   \        (bvXor 8 (at 16 (Vec 8 Bool) aes1 atIdx) \
//   \           (arrayLookupUnint arr \
//   \              (bvAdd 64 z1 z2))) \
//   \        (ite (Vec 8 Bool) b (slice Bool n1 8 n2 v1) \
//   \           (slice Bool n1 8 n2 v2))))");

let slice_28_32_4 = parse_core "\\(x : Vec 64 Bool) -> slice Bool 28 32 4 x";

natToInt_0  <- prove_print z3 (parse_core "\\(seq : Vec 12 (Vec 8 Bool)) -> bvEq 8 (at 12 (Vec 8 Bool) seq (intToNat (natToInt 0)))  (at 12 (Vec 8 Bool) seq 0)");
natToInt_1  <- prove_print z3 (parse_core "\\(seq : Vec 12 (Vec 8 Bool)) -> bvEq 8 (at 12 (Vec 8 Bool) seq (intToNat (natToInt 1)))  (at 12 (Vec 8 Bool) seq 1)");
natToInt_2  <- prove_print z3 (parse_core "\\(seq : Vec 12 (Vec 8 Bool)) -> bvEq 8 (at 12 (Vec 8 Bool) seq (intToNat (natToInt 2)))  (at 12 (Vec 8 Bool) seq 2)");
natToInt_3  <- prove_print z3 (parse_core "\\(seq : Vec 12 (Vec 8 Bool)) -> bvEq 8 (at 12 (Vec 8 Bool) seq (intToNat (natToInt 3)))  (at 12 (Vec 8 Bool) seq 3)");
natToInt_4  <- prove_print z3 (parse_core "\\(seq : Vec 12 (Vec 8 Bool)) -> bvEq 8 (at 12 (Vec 8 Bool) seq (intToNat (natToInt 4)))  (at 12 (Vec 8 Bool) seq 4)");
natToInt_5  <- prove_print z3 (parse_core "\\(seq : Vec 12 (Vec 8 Bool)) -> bvEq 8 (at 12 (Vec 8 Bool) seq (intToNat (natToInt 5)))  (at 12 (Vec 8 Bool) seq 5)");
natToInt_6  <- prove_print z3 (parse_core "\\(seq : Vec 12 (Vec 8 Bool)) -> bvEq 8 (at 12 (Vec 8 Bool) seq (intToNat (natToInt 6)))  (at 12 (Vec 8 Bool) seq 6)");
natToInt_7  <- prove_print z3 (parse_core "\\(seq : Vec 12 (Vec 8 Bool)) -> bvEq 8 (at 12 (Vec 8 Bool) seq (intToNat (natToInt 7)))  (at 12 (Vec 8 Bool) seq 7)");
natToInt_8  <- prove_print z3 (parse_core "\\(seq : Vec 12 (Vec 8 Bool)) -> bvEq 8 (at 12 (Vec 8 Bool) seq (intToNat (natToInt 8)))  (at 12 (Vec 8 Bool) seq 8)");
natToInt_9  <- prove_print z3 (parse_core "\\(seq : Vec 12 (Vec 8 Bool)) -> bvEq 8 (at 12 (Vec 8 Bool) seq (intToNat (natToInt 9)))  (at 12 (Vec 8 Bool) seq 9)");
natToInt_10 <- prove_print z3 (parse_core "\\(seq : Vec 12 (Vec 8 Bool)) -> bvEq 8 (at 12 (Vec 8 Bool) seq (intToNat (natToInt 10))) (at 12 (Vec 8 Bool) seq 10)");
natToInt_11 <- prove_print z3 (parse_core "\\(seq : Vec 12 (Vec 8 Bool)) -> bvEq 8 (at 12 (Vec 8 Bool) seq (intToNat (natToInt 11))) (at 12 (Vec 8 Bool) seq 11)");
natToInt_bonus <- prove_print z3 (parse_core "\\(seq : Vec 32 Bool) -> boolEq (at 32 Bool seq (intToNat (natToInt 0))) (at 32 Bool seq 0)");

let natToInt_lemmas =
  [ natToInt_0
  , natToInt_1
  , natToInt_2
  , natToInt_3
  , natToInt_4
  , natToInt_5
  , natToInt_6
  , natToInt_7
  , natToInt_8
  , natToInt_9
  , natToInt_10
  , natToInt_11
  , natToInt_bonus
  ];

intToLe_0  <- prove_print z3 (parse_core "boolEq (intLe (natToInt 0) (natToInt 0)) True");
intToLe_1  <- prove_print z3 (parse_core "boolEq (intLe (natToInt 0) (natToInt 1)) True");
intToLe_2  <- prove_print z3 (parse_core "boolEq (intLe (natToInt 0) (natToInt 2)) True");
intToLe_3  <- prove_print z3 (parse_core "boolEq (intLe (natToInt 0) (natToInt 3)) True");
intToLe_4  <- prove_print z3 (parse_core "boolEq (intLe (natToInt 0) (natToInt 4)) True");
intToLe_5  <- prove_print z3 (parse_core "boolEq (intLe (natToInt 0) (natToInt 5)) True");
intToLe_6  <- prove_print z3 (parse_core "boolEq (intLe (natToInt 0) (natToInt 6)) True");
intToLe_7  <- prove_print z3 (parse_core "boolEq (intLe (natToInt 0) (natToInt 7)) True");
intToLe_8  <- prove_print z3 (parse_core "boolEq (intLe (natToInt 0) (natToInt 8)) True");
intToLe_9  <- prove_print z3 (parse_core "boolEq (intLe (natToInt 0) (natToInt 9)) True");
intToLe_10 <- prove_print z3 (parse_core "boolEq (intLe (natToInt 0) (natToInt 10)) True");
intToLe_11 <- prove_print z3 (parse_core "boolEq (intLe (natToInt 0) (natToInt 11)) True");
ite_True_Vec_8 <- prove_print z3 (parse_core "\\(t f : Vec 8 Bool) -> bvEq 8 (ite (Vec 8 Bool) True t f) t");
ite_True_Bool <- prove_print z3 (parse_core "\\(t f : Bool) -> boolEq (ite Bool True t f) t");

let intLe_lemmas =
  [ intToLe_0
  , intToLe_1
  , intToLe_2
  , intToLe_3
  , intToLe_4
  , intToLe_5
  , intToLe_6
  , intToLe_7
  , intToLe_8
  , intToLe_9
  , intToLe_10
  , intToLe_11
  , ite_True_Vec_8
  , ite_True_Bool
  ];

// lolwut <- prove_print assume_unsat // (w4_unint_z3 ["aes_hw_encrypt"])
//   (beta_reduce_term (unfold_term ["ecEq", "PEqWord", "PEqVec"] (rewrite (addsimps (concat natToInt_lemmas intLe_lemmas) (cryptol_ss ()))
//   {{ \(len : [28]) (ctx_len : [60]) (iv : [12][8]) (key : [32][8]) ->
//      (lhs == rhs
//        where
//          lhs = aes_hw_encrypt
//                  [ slice_0_8_120 x102
//                  , slice_8_8_112 x102
//                  , slice_16_8_104 x102
//                  , slice_24_8_96 x102
//                  , slice_32_8_88 x102
//                  , slice_40_8_80 x102
//                  , slice_48_8_72 x102
//                  , slice_56_8_64 x102
//                  , slice_64_8_56 x102
//                  , slice_72_8_48 x102
//                  , slice_80_8_40 x102
//                  , slice_88_8_32 x102
//                  , if x69 then (slice_96_8_24 x102) else
//                      (slice_0_8_24 x104)
//                  , if x69 then (slice_104_8_16 x102) else
//                      (slice_8_8_16 x104)
//                  , if x69 then (slice_112_8_8 x102) else
//                      (slice_16_8_8 x104)
//                  , if x69 then (slice_120_8_0 x102) else
//                      (slice_24_8_0 x104) ]
//                  key
//          rhs = aes_hw_encrypt
//                  [ x74
//                  , x75
//                  , x76
//                  , x77
//                  , x78
//                  , x79
//                  , x80
//                  , x81
//                  , x82
//                  , x83
//                  , x84
//                  , x85
//                  , slice_0_8_24 x99
//                  , slice_8_8_16 x99
//                  , slice_16_8_8 x99
//                  , slice_24_8_0 x99 ]
//                  key
//
//          x7 : [64]
//          x7 = 0
//
//          x8 : [64]
//          x8 = 16
//
//          x9 : [32]
//          x9 = 1
//
//          x10 : [32]
//          x10 = 0
//
//          x17 : [64]
//          x17 = 6
//
//          x26 : [64]
//          x26 = 15
//
//          x40 : [64]
//          x40 = 288
//
//          x41 : [64]
//          x41 = 96
//
//          x43 : [64]
//          x43 = 18446744073709551615
//
//          x46 : [64]
//          x46 = 18446744073709551602
//
//          x47 : [32]
//          x47 = len # (14 : [4])
//
//          x48 : [64]
//          x48 = 18446744073709551601
//
//          x50 : [64]
//          x50 = (if (x47 @ 0) then (4294967295 : [32]) else x10) #
//                  x47
//
//          x51 : Bit
//          x51 = x7 == (x46 + x50)
//
//          x52 : [64]
//          x52 = if x51 then x46 else x48
//
//          x53 : [64]
//          x53 = x50 + x52
//
//          x54 : Bit
//          x54 = x53 < x40
//
//          x55 : [64]
//          x55 = if x54 then x7 else (x41 * (x53 / x41))
//
//          x56 : [64]
//          x56 = x43 * x55
//
//          x57 : [64]
//          x57 = x53 + x56
//
//          x60 : [64]
//          x60 = ctx_len # (1 : [4])
//
//          x67 : [64]
//          x67 = x57 / x8
//
//          x68 : [64]
//          x68 = x8 * x67
//
//          x69 : Bit
//          x69 = x7 == x68
//
//          x74 = iv @ 0
//          x75 = iv @ 1
//          x76 = iv @ 2
//          x77 = iv @ 3
//          x78 = iv @ 4
//          x79 = iv @ 5
//          x80 = iv @ 6
//          x81 = iv @ 7
//          x82 = iv @ 8
//          x83 = iv @ 9
//          x84 = iv @ 10
//          x85 = iv @ 11
//
//          x87 : [64]
//          x87 = x26 + x60
//
//          x88 : [32]
//          x88 = slice_32_32_0 (x87 / x8)
//
//          x91 : [32]
//          x91 = (2 : [32]) + x88
//
//          x92 : [8]
//          x92 = slice_0_8_24 x91
//
//          x93 : [8]
//          x93 = slice_8_8_16 x91
//
//          x94 : [8]
//          x94 = slice_16_8_8 x91
//
//          x95 : [8]
//          x95 = slice_24_8_0 x91
//
//          x98 : [96]
//          x98 = ((((((((((x74 # x75) # x76) # x77) # x78) # x79) # x80) # x81) # x82) # x83) # x84) # x85
//
//          x99 : [32]
//          x99 = x9 + slice_32_32_0 ((x87 + x50) / x8)
//
//          x101 : [64]
//          x101 = x53 / x8
//
//          x102 : [128]
//          x102 = if x54
//                   then
//                     (((x98 # x92) # x93) # x94) # x95
//                   else
//                     x98 # (x91 + slice_32_32_0 (x17 * (x101 / x17)))
//
//          x103 : [32]
//          x103 = slice_96_32_0 x102
//
//          x104 : [32]
//          x104 = x103 + (slice_28_32_4 x57))
//   }})));
//
// print lolwut;

EncryptUpdate_slice_thm_0 <- prove_print z3
  (rewrite (cryptol_ss ()) (unfold_term ["ecEq"]
  {{ \(b : Bit) (x0 : [8]) (x1 : [8]) (x2 : [8]) (x3 : [8]) (x4 : [8]) (x5 : [8]) (x6 : [8]) (x7 : [8]) (x8 : [8]) (x9 : [8]) (x10 : [8]) (x11 : [8]) (rest8_0 : [8]) (rest8_1 : [8]) (rest8_2 : [8]) (rest8_3 : [8])  (rest32 : [32]) ->
     (slice_0_8_120 (if b then ((((x0ThruX11 # rest8_0) # rest8_1) # rest8_2) # rest8_3) else (x0ThruX11 # rest32)) == x0
       where
         x0ThruX11 : [96]
         x0ThruX11 = ((((((((((x0 # x1) # x2) # x3) # x4) # x5) # x6) # x7) # x8) # x9) # x10) # x11
 )}}));
EncryptUpdate_slice_thm_1 <- prove_print z3
  (rewrite (cryptol_ss ()) (unfold_term ["ecEq"]
  {{ \(b : Bit) (x0 : [8]) (x1 : [8]) (x2 : [8]) (x3 : [8]) (x4 : [8]) (x5 : [8]) (x6 : [8]) (x7 : [8]) (x8 : [8]) (x9 : [8]) (x10 : [8]) (x11 : [8]) (rest8_0 : [8]) (rest8_1 : [8]) (rest8_2 : [8]) (rest8_3 : [8])  (rest32 : [32]) ->
     (slice_8_8_112 (if b then ((((x0ThruX11 # rest8_0) # rest8_1) # rest8_2) # rest8_3) else (x0ThruX11 # rest32)) == x1
       where
         x0ThruX11 : [96]
         x0ThruX11 = ((((((((((x0 # x1) # x2) # x3) # x4) # x5) # x6) # x7) # x8) # x9) # x10) # x11
 )}}));
EncryptUpdate_slice_thm_2 <- prove_print z3
  (rewrite (cryptol_ss ()) (unfold_term ["ecEq"]
  {{ \(b : Bit) (x0 : [8]) (x1 : [8]) (x2 : [8]) (x3 : [8]) (x4 : [8]) (x5 : [8]) (x6 : [8]) (x7 : [8]) (x8 : [8]) (x9 : [8]) (x10 : [8]) (x11 : [8]) (rest8_0 : [8]) (rest8_1 : [8]) (rest8_2 : [8]) (rest8_3 : [8])  (rest32 : [32]) ->
     (slice_16_8_104 (if b then ((((x0ThruX11 # rest8_0) # rest8_1) # rest8_2) # rest8_3) else (x0ThruX11 # rest32)) == x2
       where
         x0ThruX11 : [96]
         x0ThruX11 = ((((((((((x0 # x1) # x2) # x3) # x4) # x5) # x6) # x7) # x8) # x9) # x10) # x11
 )}}));
EncryptUpdate_slice_thm_3 <- prove_print z3
  (rewrite (cryptol_ss ()) (unfold_term ["ecEq"]
  {{ \(b : Bit) (x0 : [8]) (x1 : [8]) (x2 : [8]) (x3 : [8]) (x4 : [8]) (x5 : [8]) (x6 : [8]) (x7 : [8]) (x8 : [8]) (x9 : [8]) (x10 : [8]) (x11 : [8]) (rest8_0 : [8]) (rest8_1 : [8]) (rest8_2 : [8]) (rest8_3 : [8])  (rest32 : [32]) ->
     (slice_24_8_96 (if b then ((((x0ThruX11 # rest8_0) # rest8_1) # rest8_2) # rest8_3) else (x0ThruX11 # rest32)) == x3
       where
         x0ThruX11 : [96]
         x0ThruX11 = ((((((((((x0 # x1) # x2) # x3) # x4) # x5) # x6) # x7) # x8) # x9) # x10) # x11
 )}}));
EncryptUpdate_slice_thm_4 <- prove_print z3
  (rewrite (cryptol_ss ()) (unfold_term ["ecEq"]
  {{ \(b : Bit) (x0 : [8]) (x1 : [8]) (x2 : [8]) (x3 : [8]) (x4 : [8]) (x5 : [8]) (x6 : [8]) (x7 : [8]) (x8 : [8]) (x9 : [8]) (x10 : [8]) (x11 : [8]) (rest8_0 : [8]) (rest8_1 : [8]) (rest8_2 : [8]) (rest8_3 : [8])  (rest32 : [32]) ->
     (slice_32_8_88 (if b then ((((x0ThruX11 # rest8_0) # rest8_1) # rest8_2) # rest8_3) else (x0ThruX11 # rest32)) == x4
       where
         x0ThruX11 : [96]
         x0ThruX11 = ((((((((((x0 # x1) # x2) # x3) # x4) # x5) # x6) # x7) # x8) # x9) # x10) # x11
 )}}));
EncryptUpdate_slice_thm_5 <- prove_print z3
  (rewrite (cryptol_ss ()) (unfold_term ["ecEq"]
  {{ \(b : Bit) (x0 : [8]) (x1 : [8]) (x2 : [8]) (x3 : [8]) (x4 : [8]) (x5 : [8]) (x6 : [8]) (x7 : [8]) (x8 : [8]) (x9 : [8]) (x10 : [8]) (x11 : [8]) (rest8_0 : [8]) (rest8_1 : [8]) (rest8_2 : [8]) (rest8_3 : [8])  (rest32 : [32]) ->
     (slice_40_8_80 (if b then ((((x0ThruX11 # rest8_0) # rest8_1) # rest8_2) # rest8_3) else (x0ThruX11 # rest32)) == x5
       where
         x0ThruX11 : [96]
         x0ThruX11 = ((((((((((x0 # x1) # x2) # x3) # x4) # x5) # x6) # x7) # x8) # x9) # x10) # x11
 )}}));
EncryptUpdate_slice_thm_6 <- prove_print z3
  (rewrite (cryptol_ss ()) (unfold_term ["ecEq"]
  {{ \(b : Bit) (x0 : [8]) (x1 : [8]) (x2 : [8]) (x3 : [8]) (x4 : [8]) (x5 : [8]) (x6 : [8]) (x7 : [8]) (x8 : [8]) (x9 : [8]) (x10 : [8]) (x11 : [8]) (rest8_0 : [8]) (rest8_1 : [8]) (rest8_2 : [8]) (rest8_3 : [8])  (rest32 : [32]) ->
     (slice_48_8_72 (if b then ((((x0ThruX11 # rest8_0) # rest8_1) # rest8_2) # rest8_3) else (x0ThruX11 # rest32)) == x6
       where
         x0ThruX11 : [96]
         x0ThruX11 = ((((((((((x0 # x1) # x2) # x3) # x4) # x5) # x6) # x7) # x8) # x9) # x10) # x11
 )}}));
EncryptUpdate_slice_thm_7 <- prove_print z3
  (rewrite (cryptol_ss ()) (unfold_term ["ecEq"]
  {{ \(b : Bit) (x0 : [8]) (x1 : [8]) (x2 : [8]) (x3 : [8]) (x4 : [8]) (x5 : [8]) (x6 : [8]) (x7 : [8]) (x8 : [8]) (x9 : [8]) (x10 : [8]) (x11 : [8]) (rest8_0 : [8]) (rest8_1 : [8]) (rest8_2 : [8]) (rest8_3 : [8])  (rest32 : [32]) ->
     (slice_56_8_64 (if b then ((((x0ThruX11 # rest8_0) # rest8_1) # rest8_2) # rest8_3) else (x0ThruX11 # rest32)) == x7
       where
         x0ThruX11 : [96]
         x0ThruX11 = ((((((((((x0 # x1) # x2) # x3) # x4) # x5) # x6) # x7) # x8) # x9) # x10) # x11
 )}}));
EncryptUpdate_slice_thm_8 <- prove_print z3
  (rewrite (cryptol_ss ()) (unfold_term ["ecEq"]
  {{ \(b : Bit) (x0 : [8]) (x1 : [8]) (x2 : [8]) (x3 : [8]) (x4 : [8]) (x5 : [8]) (x6 : [8]) (x7 : [8]) (x8 : [8]) (x9 : [8]) (x10 : [8]) (x11 : [8]) (rest8_0 : [8]) (rest8_1 : [8]) (rest8_2 : [8]) (rest8_3 : [8])  (rest32 : [32]) ->
     (slice_64_8_56 (if b then ((((x0ThruX11 # rest8_0) # rest8_1) # rest8_2) # rest8_3) else (x0ThruX11 # rest32)) == x8
       where
         x0ThruX11 : [96]
         x0ThruX11 = ((((((((((x0 # x1) # x2) # x3) # x4) # x5) # x6) # x7) # x8) # x9) # x10) # x11
 )}}));
EncryptUpdate_slice_thm_9 <- prove_print z3
  (rewrite (cryptol_ss ()) (unfold_term ["ecEq"]
  {{ \(b : Bit) (x0 : [8]) (x1 : [8]) (x2 : [8]) (x3 : [8]) (x4 : [8]) (x5 : [8]) (x6 : [8]) (x7 : [8]) (x8 : [8]) (x9 : [8]) (x10 : [8]) (x11 : [8]) (rest8_0 : [8]) (rest8_1 : [8]) (rest8_2 : [8]) (rest8_3 : [8])  (rest32 : [32]) ->
     (slice_72_8_48 (if b then ((((x0ThruX11 # rest8_0) # rest8_1) # rest8_2) # rest8_3) else (x0ThruX11 # rest32)) == x9
       where
         x0ThruX11 : [96]
         x0ThruX11 = ((((((((((x0 # x1) # x2) # x3) # x4) # x5) # x6) # x7) # x8) # x9) # x10) # x11
 )}}));
EncryptUpdate_slice_thm_10 <- prove_print z3
  (rewrite (cryptol_ss ()) (unfold_term ["ecEq"]
  {{ \(b : Bit) (x0 : [8]) (x1 : [8]) (x2 : [8]) (x3 : [8]) (x4 : [8]) (x5 : [8]) (x6 : [8]) (x7 : [8]) (x8 : [8]) (x9 : [8]) (x10 : [8]) (x11 : [8]) (rest8_0 : [8]) (rest8_1 : [8]) (rest8_2 : [8]) (rest8_3 : [8])  (rest32 : [32]) ->
     (slice_80_8_40 (if b then ((((x0ThruX11 # rest8_0) # rest8_1) # rest8_2) # rest8_3) else (x0ThruX11 # rest32)) == x10
       where
         x0ThruX11 : [96]
         x0ThruX11 = ((((((((((x0 # x1) # x2) # x3) # x4) # x5) # x6) # x7) # x8) # x9) # x10) # x11
 )}}));
EncryptUpdate_slice_thm_11 <- prove_print z3
  (rewrite (cryptol_ss ()) (unfold_term ["ecEq"]
  {{ \(b : Bit) (x0 : [8]) (x1 : [8]) (x2 : [8]) (x3 : [8]) (x4 : [8]) (x5 : [8]) (x6 : [8]) (x7 : [8]) (x8 : [8]) (x9 : [8]) (x10 : [8]) (x11 : [8]) (rest8_0 : [8]) (rest8_1 : [8]) (rest8_2 : [8]) (rest8_3 : [8])  (rest32 : [32]) ->
     (slice_88_8_32 (if b then ((((x0ThruX11 # rest8_0) # rest8_1) # rest8_2) # rest8_3) else (x0ThruX11 # rest32)) == x11
       where
         x0ThruX11 : [96]
         x0ThruX11 = ((((((((((x0 # x1) # x2) # x3) # x4) # x5) # x6) # x7) # x8) # x9) # x10) # x11
 )}}));

EncryptUpdate_slice_thm_12 <- prove_print assume_unsat
  (rewrite (addsimps (concat natToInt_lemmas intLe_lemmas) (cryptol_ss ())) (unfold_term ["ecEq"]
  {{ \(len : [28]) (ctx_len : [60]) (iv : [12][8]) ->
     ((if x69 then (slice_96_8_24 x102) else (slice_0_8_24 x104)) == slice_0_8_24 x99
       where
         x7 : [64]
         x7 = 0

         x8 : [64]
         x8 = 16

         x9 : [32]
         x9 = 1

         x10 : [32]
         x10 = 0

         x17 : [64]
         x17 = 6

         x26 : [64]
         x26 = 15

         x40 : [64]
         x40 = 288

         x41 : [64]
         x41 = 96

         x43 : [64]
         x43 = 18446744073709551615

         x46 : [64]
         x46 = 18446744073709551602

         x47 : [32]
         x47 = len # (14 : [4])

         x48 : [64]
         x48 = 18446744073709551601

         x50 : [64]
         x50 = (if (x47 @ 0) then (4294967295 : [32]) else x10) #
                 x47

         x51 : Bit
         x51 = x7 == (x46 + x50)

         x52 : [64]
         x52 = if x51 then x46 else x48

         x53 : [64]
         x53 = x50 + x52

         x54 : Bit
         x54 = x53 < x40

         x55 : [64]
         x55 = if x54 then x7 else (x41 * (x53 / x41))

         x56 : [64]
         x56 = x43 * x55

         x57 : [64]
         x57 = x53 + x56

         x60 : [64]
         x60 = ctx_len # (1 : [4])

         x67 : [64]
         x67 = x57 / x8

         x68 : [64]
         x68 = x8 * x67

         x69 : Bit
         x69 = x7 == x68

         x74 = iv @ 0
         x75 = iv @ 1
         x76 = iv @ 2
         x77 = iv @ 3
         x78 = iv @ 4
         x79 = iv @ 5
         x80 = iv @ 6
         x81 = iv @ 7
         x82 = iv @ 8
         x83 = iv @ 9
         x84 = iv @ 10
         x85 = iv @ 11

         x87 : [64]
         x87 = x26 + x60

         x88 : [32]
         x88 = slice_32_32_0 (x87 / x8)

         x91 : [32]
         x91 = (2 : [32]) + x88

         x92 : [8]
         x92 = slice_0_8_24 x91

         x93 : [8]
         x93 = slice_8_8_16 x91

         x94 : [8]
         x94 = slice_16_8_8 x91

         x95 : [8]
         x95 = slice_24_8_0 x91

         x98 : [96]
         x98 = ((((((((((x74 # x75) # x76) # x77) # x78) # x79) # x80) # x81) # x82) # x83) # x84) # x85

         x99 : [32]
         x99 = x9 + slice_32_32_0 ((x87 + x50) / x8)

         x101 : [64]
         x101 = x53 / x8

         x102 : [128]
         x102 = if x54
                  then
                    (((x98 # x92) # x93) # x94) # x95
                  else
                    x98 # (x91 + slice_32_32_0 (x17 * (x101 / x17)))

         x103 : [32]
         x103 = slice_96_32_0 x102

         x104 : [32]
         x104 = x103 + (slice_28_32_4 x57)
 )}}));
EncryptUpdate_slice_thm_13 <- prove_print assume_unsat
  (rewrite (addsimps (concat natToInt_lemmas intLe_lemmas) (cryptol_ss ())) (unfold_term ["ecEq"]
  {{ \(len : [28]) (ctx_len : [60]) (iv : [12][8]) ->
     ((if x69 then (slice_104_8_16 x102) else (slice_8_8_16 x104)) == slice_8_8_16 x99
       where
         x7 : [64]
         x7 = 0

         x8 : [64]
         x8 = 16

         x9 : [32]
         x9 = 1

         x10 : [32]
         x10 = 0

         x17 : [64]
         x17 = 6

         x26 : [64]
         x26 = 15

         x40 : [64]
         x40 = 288

         x41 : [64]
         x41 = 96

         x43 : [64]
         x43 = 18446744073709551615

         x46 : [64]
         x46 = 18446744073709551602

         x47 : [32]
         x47 = len # (14 : [4])

         x48 : [64]
         x48 = 18446744073709551601

         x50 : [64]
         x50 = (if (x47 @ 0) then (4294967295 : [32]) else x10) #
                 x47

         x51 : Bit
         x51 = x7 == (x46 + x50)

         x52 : [64]
         x52 = if x51 then x46 else x48

         x53 : [64]
         x53 = x50 + x52

         x54 : Bit
         x54 = x53 < x40

         x55 : [64]
         x55 = if x54 then x7 else (x41 * (x53 / x41))

         x56 : [64]
         x56 = x43 * x55

         x57 : [64]
         x57 = x53 + x56

         x60 : [64]
         x60 = ctx_len # (1 : [4])

         x67 : [64]
         x67 = x57 / x8

         x68 : [64]
         x68 = x8 * x67

         x69 : Bit
         x69 = x7 == x68

         x74 = iv @ 0
         x75 = iv @ 1
         x76 = iv @ 2
         x77 = iv @ 3
         x78 = iv @ 4
         x79 = iv @ 5
         x80 = iv @ 6
         x81 = iv @ 7
         x82 = iv @ 8
         x83 = iv @ 9
         x84 = iv @ 10
         x85 = iv @ 11

         x87 : [64]
         x87 = x26 + x60

         x88 : [32]
         x88 = slice_32_32_0 (x87 / x8)

         x91 : [32]
         x91 = (2 : [32]) + x88

         x92 : [8]
         x92 = slice_0_8_24 x91

         x93 : [8]
         x93 = slice_8_8_16 x91

         x94 : [8]
         x94 = slice_16_8_8 x91

         x95 : [8]
         x95 = slice_24_8_0 x91

         x98 : [96]
         x98 = ((((((((((x74 # x75) # x76) # x77) # x78) # x79) # x80) # x81) # x82) # x83) # x84) # x85

         x99 : [32]
         x99 = x9 + slice_32_32_0 ((x87 + x50) / x8)

         x101 : [64]
         x101 = x53 / x8

         x102 : [128]
         x102 = if x54
                  then
                    (((x98 # x92) # x93) # x94) # x95
                  else
                    x98 # (x91 + slice_32_32_0 (x17 * (x101 / x17)))

         x103 : [32]
         x103 = slice_96_32_0 x102

         x104 : [32]
         x104 = x103 + (slice_28_32_4 x57)
 )}}));
EncryptUpdate_slice_thm_14 <- prove_print assume_unsat
  (rewrite (addsimps (concat natToInt_lemmas intLe_lemmas) (cryptol_ss ())) (unfold_term ["ecEq"]
  {{ \(len : [28]) (ctx_len : [60]) (iv : [12][8]) ->
     ((if x69 then (slice_112_8_8 x102) else (slice_16_8_8 x104)) == slice_16_8_8 x99
       where
         x7 : [64]
         x7 = 0

         x8 : [64]
         x8 = 16

         x9 : [32]
         x9 = 1

         x10 : [32]
         x10 = 0

         x17 : [64]
         x17 = 6

         x26 : [64]
         x26 = 15

         x40 : [64]
         x40 = 288

         x41 : [64]
         x41 = 96

         x43 : [64]
         x43 = 18446744073709551615

         x46 : [64]
         x46 = 18446744073709551602

         x47 : [32]
         x47 = len # (14 : [4])

         x48 : [64]
         x48 = 18446744073709551601

         x50 : [64]
         x50 = (if (x47 @ 0) then (4294967295 : [32]) else x10) #
                 x47

         x51 : Bit
         x51 = x7 == (x46 + x50)

         x52 : [64]
         x52 = if x51 then x46 else x48

         x53 : [64]
         x53 = x50 + x52

         x54 : Bit
         x54 = x53 < x40

         x55 : [64]
         x55 = if x54 then x7 else (x41 * (x53 / x41))

         x56 : [64]
         x56 = x43 * x55

         x57 : [64]
         x57 = x53 + x56

         x60 : [64]
         x60 = ctx_len # (1 : [4])

         x67 : [64]
         x67 = x57 / x8

         x68 : [64]
         x68 = x8 * x67

         x69 : Bit
         x69 = x7 == x68

         x74 = iv @ 0
         x75 = iv @ 1
         x76 = iv @ 2
         x77 = iv @ 3
         x78 = iv @ 4
         x79 = iv @ 5
         x80 = iv @ 6
         x81 = iv @ 7
         x82 = iv @ 8
         x83 = iv @ 9
         x84 = iv @ 10
         x85 = iv @ 11

         x87 : [64]
         x87 = x26 + x60

         x88 : [32]
         x88 = slice_32_32_0 (x87 / x8)

         x91 : [32]
         x91 = (2 : [32]) + x88

         x92 : [8]
         x92 = slice_0_8_24 x91

         x93 : [8]
         x93 = slice_8_8_16 x91

         x94 : [8]
         x94 = slice_16_8_8 x91

         x95 : [8]
         x95 = slice_24_8_0 x91

         x98 : [96]
         x98 = ((((((((((x74 # x75) # x76) # x77) # x78) # x79) # x80) # x81) # x82) # x83) # x84) # x85

         x99 : [32]
         x99 = x9 + slice_32_32_0 ((x87 + x50) / x8)

         x101 : [64]
         x101 = x53 / x8

         x102 : [128]
         x102 = if x54
                  then
                    (((x98 # x92) # x93) # x94) # x95
                  else
                    x98 # (x91 + slice_32_32_0 (x17 * (x101 / x17)))

         x103 : [32]
         x103 = slice_96_32_0 x102

         x104 : [32]
         x104 = x103 + (slice_28_32_4 x57)
 )}}));
EncryptUpdate_slice_thm_15 <- prove_print assume_unsat
  (rewrite (addsimps (concat natToInt_lemmas intLe_lemmas) (cryptol_ss ())) (unfold_term ["ecEq"]
  {{ \(len : [28]) (ctx_len : [60]) (iv : [12][8]) ->
     ((if x69 then (slice_120_8_0 x102) else (slice_24_8_0 x104)) == slice_24_8_0 x99
       where
         x7 : [64]
         x7 = 0

         x8 : [64]
         x8 = 16

         x9 : [32]
         x9 = 1

         x10 : [32]
         x10 = 0

         x17 : [64]
         x17 = 6

         x26 : [64]
         x26 = 15

         x40 : [64]
         x40 = 288

         x41 : [64]
         x41 = 96

         x43 : [64]
         x43 = 18446744073709551615

         x46 : [64]
         x46 = 18446744073709551602

         x47 : [32]
         x47 = len # (14 : [4])

         x48 : [64]
         x48 = 18446744073709551601

         x50 : [64]
         x50 = (if (x47 @ 0) then (4294967295 : [32]) else x10) #
                 x47

         x51 : Bit
         x51 = x7 == (x46 + x50)

         x52 : [64]
         x52 = if x51 then x46 else x48

         x53 : [64]
         x53 = x50 + x52

         x54 : Bit
         x54 = x53 < x40

         x55 : [64]
         x55 = if x54 then x7 else (x41 * (x53 / x41))

         x56 : [64]
         x56 = x43 * x55

         x57 : [64]
         x57 = x53 + x56

         x60 : [64]
         x60 = ctx_len # (1 : [4])

         x67 : [64]
         x67 = x57 / x8

         x68 : [64]
         x68 = x8 * x67

         x69 : Bit
         x69 = x7 == x68

         x74 = iv @ 0
         x75 = iv @ 1
         x76 = iv @ 2
         x77 = iv @ 3
         x78 = iv @ 4
         x79 = iv @ 5
         x80 = iv @ 6
         x81 = iv @ 7
         x82 = iv @ 8
         x83 = iv @ 9
         x84 = iv @ 10
         x85 = iv @ 11

         x87 : [64]
         x87 = x26 + x60

         x88 : [32]
         x88 = slice_32_32_0 (x87 / x8)

         x91 : [32]
         x91 = (2 : [32]) + x88

         x92 : [8]
         x92 = slice_0_8_24 x91

         x93 : [8]
         x93 = slice_8_8_16 x91

         x94 : [8]
         x94 = slice_16_8_8 x91

         x95 : [8]
         x95 = slice_24_8_0 x91

         x98 : [96]
         x98 = ((((((((((x74 # x75) # x76) # x77) # x78) # x79) # x80) # x81) # x82) # x83) # x84) # x85

         x99 : [32]
         x99 = x9 + slice_32_32_0 ((x87 + x50) / x8)

         x101 : [64]
         x101 = x53 / x8

         x102 : [128]
         x102 = if x54
                  then
                    (((x98 # x92) # x93) # x94) # x95
                  else
                    x98 # (x91 + slice_32_32_0 (x17 * (x101 / x17)))

         x103 : [32]
         x103 = slice_96_32_0 x102

         x104 : [32]
         x104 = x103 + (slice_28_32_4 x57)
 )}}));

let EncryptUpdate_slice_thms =
  [ EncryptUpdate_slice_thm_0
  , EncryptUpdate_slice_thm_1
  , EncryptUpdate_slice_thm_2
  , EncryptUpdate_slice_thm_3
  , EncryptUpdate_slice_thm_4
  , EncryptUpdate_slice_thm_5
  , EncryptUpdate_slice_thm_6
  , EncryptUpdate_slice_thm_7
  , EncryptUpdate_slice_thm_8
  , EncryptUpdate_slice_thm_9
  , EncryptUpdate_slice_thm_10
  , EncryptUpdate_slice_thm_11

  , EncryptUpdate_slice_thm_12
  , EncryptUpdate_slice_thm_13
  , EncryptUpdate_slice_thm_14
  , EncryptUpdate_slice_thm_15
  ];

let do_prove = true;

llvm_verify m "EVP_EncryptUpdate"
  [ aes_gcm_from_cipher_ctx_ov
  , aes_hw_encrypt_ov
  , aes_hw_ctr32_encrypt_blocks_bounded_array_ov
  , gcm_gmult_avx_ov
  , gcm_ghash_avx_bounded_array_ov
  , aesni_gcm_encrypt_array_ov
  , aesni_gcm_decrypt_array_ov
  ]
  true
  (EVP_CipherUpdate_array_spec {{ 1 : [32] }} 1 15)
  (do {
    simplify (cryptol_ss ());
    goal_eval_unint ["aesni_gcm_encrypt", "aes_ctr32_encrypt_blocks_array", "aes_hw_encrypt", "gcm_ghash_blocks_array", "gcm_polyval", "pmult", "pmod"];
    simplify (addsimps [aesni_gcm_encrypt_Yi_thm] empty_ss);
    goal_eval_unint ["aesni_gcm_encrypt", "aes_ctr32_encrypt_blocks_array", "aes_hw_encrypt", "gcm_ghash_blocks_array", "gcm_polyval", "pmult", "pmod"];
    simplify (addsimps [bvand_bvudiv_thm, bvudiv_bvmul_bvudiv_thm, bvurem_16_append_thm] basic_ss);
    simplify (addsimps [arrayLookupUnint_thm, arrayUpdateUnint_thm, arrayCopyUnint_thm, arrayConstantUnint_thm] empty_ss);
    simplify (addsimps add_xor_slice_thms basic_ss);
    goal_eval_unint ["aesni_gcm_encrypt", "aes_ctr32_encrypt_blocks_array", "aes_hw_encrypt", "gcm_ghash_blocks_array", "gcm_polyval", "pmult", "pmod", "arrayLookupUnint", "arrayUpdateUnint", "arrayCopyUnint", "arrayConstantUnint"];
    simplify (addsimps [ite_bveq_0_thm, ite_bveq_1_thm, bveq_ite_bv8_0_thm, bveq_ite_bv8_1_thm, arrayeq_ite_0_thm, arrayeq_ite_1_thm, foo_thm, bar_thm] basic_ss);
    goal_eval_unint ["aesni_gcm_encrypt", "aes_ctr32_encrypt_blocks_array", "aes_hw_encrypt", "gcm_ghash_blocks_array", "gcm_polyval", "pmult", "pmod", "arrayLookupUnint", "arrayUpdateUnint", "arrayCopyUnint", "arrayConstantUnint"];
    print_goal;
    is_out_post <- goal_has_some_tag ["output buffer postcondition"];
    is_Xi_post <- goal_has_some_tag ["Xi postcondition"];
    if is_out_post then do {
      assume_unsat;
      // w4_unint_yices ["aesni_gcm_encrypt", "aes_ctr32_encrypt_blocks_array", "aes_hw_encrypt", "gcm_ghash_blocks_array", "gcm_polyval", "pmult", "pmod", "arrayLookupUnint", "arrayUpdateUnint", "arrayCopyUnint", "arrayConstantUnint"];
    } else if is_Xi_post then do {
      simplify (addsimps EncryptUpdate_slice_thms empty_ss);
      print_goal;
      w4_unint_z3_using "qfufbv" ["aesni_gcm_encrypt", "aes_ctr32_encrypt_blocks_array", "aes_hw_encrypt", "gcm_ghash_blocks_array", "gcm_polyval", "pmult", "pmod", "arrayLookupUnint", "arrayUpdateUnint", "arrayCopyUnint", "arrayConstantUnint"];
    } else do {
      assume_unsat;
      // w4_unint_z3 ["aesni_gcm_encrypt", "aes_ctr32_encrypt_blocks_array", "aes_hw_encrypt", "gcm_ghash_blocks_array", "gcm_polyval", "pmult", "pmod", "arrayLookupUnint", "arrayUpdateUnint", "arrayCopyUnint", "arrayConstantUnint"];
    };
  });

let do_prove = false;

llvm_verify m "EVP_DecryptUpdate"
  [ aes_gcm_from_cipher_ctx_ov
  , aes_hw_encrypt_ov
  , aes_hw_ctr32_encrypt_blocks_bounded_array_ov
  , gcm_gmult_avx_ov
  , gcm_ghash_avx_bounded_array_ov
  , aesni_gcm_encrypt_array_ov
  , aesni_gcm_decrypt_array_ov
  ]
  true
  (EVP_CipherUpdate_array_spec {{ 0 : [32] }} 1 15)
  (do {
    simplify (cryptol_ss ());
    goal_eval_unint ["aesni_gcm_decrypt", "aes_ctr32_encrypt_blocks_array", "aes_hw_encrypt", "gcm_ghash_blocks_array", "gcm_polyval", "pmult", "pmod"];
    simplify (addsimps [aesni_gcm_decrypt_Yi_thm] empty_ss);
    goal_eval_unint ["aesni_gcm_decrypt", "aes_ctr32_encrypt_blocks_array", "aes_hw_encrypt", "gcm_ghash_blocks_array", "gcm_polyval", "pmult", "pmod"];
    simplify (addsimps [bvand_bvudiv_thm, bvudiv_bvmul_bvudiv_thm, bvurem_16_append_thm] basic_ss);
    simplify (addsimps [arrayLookupUnint_thm, arrayUpdateUnint_thm, arrayCopyUnint_thm, arrayConstantUnint_thm] empty_ss);
    simplify (addsimps add_xor_slice_thms basic_ss);
    goal_eval_unint ["aesni_gcm_decrypt", "aes_ctr32_encrypt_blocks_array", "aes_hw_encrypt", "gcm_ghash_blocks_array", "gcm_polyval", "pmult", "pmod", "arrayLookupUnint", "arrayUpdateUnint", "arrayCopyUnint", "arrayConstantUnint"];
    simplify (addsimps [ite_bveq_0_thm, ite_bveq_1_thm, bveq_ite_bv8_0_thm, bveq_ite_bv8_1_thm, arrayeq_ite_0_thm, arrayeq_ite_1_thm, foo_thm, bar_thm] basic_ss);
    goal_eval_unint ["aesni_gcm_decrypt", "aes_ctr32_encrypt_blocks_array", "aes_hw_encrypt", "gcm_ghash_blocks_array", "gcm_polyval", "pmult", "pmod", "arrayLookupUnint", "arrayUpdateUnint", "arrayCopyUnint", "arrayConstantUnint"];
    print_goal;
    is_out_post <- goal_has_some_tag ["output buffer postcondition"];
    is_Xi_post <- goal_has_some_tag ["Xi postcondition"];
    if is_out_post then do {
      w4_unint_yices ["aesni_gcm_decrypt", "aes_ctr32_encrypt_blocks_array", "aes_hw_encrypt", "gcm_ghash_blocks_array", "gcm_polyval", "pmult", "pmod", "arrayLookupUnint", "arrayUpdateUnint", "arrayCopyUnint", "arrayConstantUnint"];
    } else if is_Xi_post then do {
      w4_unint_z3_using "qfufbv" ["aesni_gcm_decrypt", "aes_ctr32_encrypt_blocks_array", "aes_hw_encrypt", "gcm_ghash_blocks_array", "gcm_polyval", "pmult", "pmod", "arrayLookupUnint", "arrayUpdateUnint", "arrayCopyUnint", "arrayConstantUnint"];
    } else do {
      w4_unint_z3 ["aesni_gcm_decrypt", "aes_ctr32_encrypt_blocks_array", "aes_hw_encrypt", "gcm_ghash_blocks_array", "gcm_polyval", "pmult", "pmod", "arrayLookupUnint", "arrayUpdateUnint", "arrayCopyUnint", "arrayConstantUnint"];
    };
  });

disable_what4_eval;
disable_what4_hash_consing;

llvm_verify m "EVP_EncryptFinal_ex"
  evp_cipher_ovs
  true
  (EVP_EncryptFinal_ex_spec evp_cipher_final_gcm_len)
  evp_cipher_tactic;

llvm_verify m "EVP_DecryptFinal_ex"
  evp_cipher_ovs
  true
  (EVP_DecryptFinal_ex_spec evp_cipher_final_gcm_len)
  evp_cipher_tactic;

